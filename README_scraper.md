# 用户信息获取器

这个脚本用于获取ACM网站的用户信息。

## 功能

- 遍历 `https://acm.shanghaitech.edu.cn/user/XXXX` 页面（XXXX为0001-1800的用户ID）
- 提取每个用户的以下信息：
  - UID（用户编号）
  - 用户名
  - 昵称
  - 电子邮件地址
- 将所有信息保存到CSV文件中

## 安装依赖

```bash
pip install -r requirements.txt
```

## 使用方法

### 基本使用

```bash
python acm_user_scraper_concurrent.py
```

这将：
- 从用户ID 0001 开始遍历到 1800
- 剔除常见广告账号（例如“发票”）
- 将结果保存到 `acm_users.csv` 文件
- 在控制台显示进度信息

### 自定义参数

你可以修改 `main()` 函数中的参数：

```python
users = scraper.scrape_all_users(
    start_uid=1,      # 起始用户ID
    max_uid=1800,     # 最大用户ID
    output_file='acm_users.csv'  # 输出文件名
)
```

## 输出格式

CSV文件包含以下列：
- `uid`: 用户ID（如0001, 0002等）
- `username`: 用户名
- `nickname`: 昵称
- `email`: 电子邮件地址

## 注意事项

1. **请求频率**: 脚本在每次请求之间有0.5秒的延时，避免对服务器造成过大压力
2. **错误处理**: 如果连续50个用户ID不存在，脚本会自动停止
3. **数据保存**: 每获取10个用户信息就会保存一次，防止数据丢失
4. **网络超时**: 每个请求的超时时间为10秒

## 可能需要调整的地方

由于网页结构可能会变化，你可能需要根据实际的HTML结构调整以下部分：

1. **用户名提取**: 修改 `get_user_info()` 方法中查找用户名的CSS选择器
2. **邮箱提取**: 调整邮箱按钮或链接的查找逻辑
3. **昵称提取**: 根据实际页面结构修改昵称的提取方式

## 法律声明

请确保你的爬虫行为符合网站的使用条款和相关法律法规。建议：
- 遵守robots.txt文件的规定
- 不要过于频繁地请求服务器
- 仅用于学习和研究目的
- 尊重用户隐私

## 故障排除

如果遇到问题：

1. **网络连接错误**: 检查网络连接和目标网站是否可访问
2. **解析错误**: 可能是网页结构发生了变化，需要更新CSS选择器
3. **权限错误**: 某些页面可能需要登录才能访问

查看控制台输出的日志信息可以帮助诊断问题。